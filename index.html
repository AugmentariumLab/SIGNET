<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>SIGNET: Efficient Neural Representations for Light Fields
    | Brandon Y. Feng Amitabh Varshney International Conference on Computer Vision (ICCV 2021) - Oral</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="SIGNET: Efficient Neural Representations for Light Fields
" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Brandon Y. Feng Amitabh Varshney International Conference on Computer Vision (ICCV 2021) - Oral" />
<meta property="og:description" content="Brandon Y. Feng Amitabh Varshney International Conference on Computer Vision (ICCV 2021) - Oral" />
<link rel="canonical" href="https://augmentariumlab.github.io/SIGNET/" />
<meta property="og:url" content="https://augmentariumlab.github.io/SIGNET/" />
<meta property="og:site_name" content="SIGNET: Efficient Neural Representations for Light Fields
" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="SIGNET: Efficient Neural Representations for Light Fields
" />
<script type="application/ld+json">
{"headline":"SIGNET: Efficient Neural Representations For Light Fields","description":"Brandon Y. Feng Amitabh Varshney International Conference on Computer Vision (ICCV 2021) Mention","url":"https://augmentariumlab.github.io/SIGNET/","@type":"WebSite","name":"SIGNET: Efficient Neural Representations For Light Fields
","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="assets/css/style.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">SIGNET: Efficient Neural Representations for Light Fields
</h1>
      <h2 class="project-tagline"><div style="display: flex; flex-direction: row; color: #FFF; flex-wrap: wrap;"> <a href="https://brandonyfeng.github.io" style="margin-left:1rem; flex-grow: 1; color: #FFF;">Brandon Y. Feng <a href="http://www.cs.umd.edu/~varshney/" style="margin-left:1rem; flex-grow: 1; color: #FFF;">Amitabh Varshney</a> </div> <p style="margin-bottom: -3rem;"> <a href="https://iccv2021.thecvf.com/" style="color: #FFF">International Conference on Computer Vision (ICCV 2021)</a><br> Oral Presentation </p></h2>
      
    </section>

    <section class="main-content">
      
<iframe width="560" height="355" src="https://www.youtube.com/embed/te4SV9Pl7Mo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" style="max-width: 100%; position: relative; left: 50%; transform: translateX(-50%);"></iframe>

<h2 id="abstract">Abstract</h2>

<!--<p><a href="https://duruofei.com/papers/Li_ALog-RectilinearTransformationForFoveated360-DegreeVideoStreaming_TVCG2021.pdf"><img src="/foveated-360-video/resources/teaser.png" alt="Teaser image of A Log-Rectilinear Transformation for Foveated 360-degree Video Streaming" /></a></p>-->

<p>  We present a novel neural representation for light field content that enables compact storage and easy local reconstruction with high fidelity. We use a fully-connected neural network to learn the mapping function between each light field pixel's coordinates and its corresponding color values. Since neural networks that simply take in raw coordinates are unable to accurately learn data containing fine details, we present an input transformation strategy based on the Gegenbauer polynomials, which previously showed theoretical advantages over the Fourier basis. We conduct experiments that show our Gegenbauer-based design combined with sinusoidal activation functions leads to a better light field reconstruction quality than a variety of network designs, including those with Fourier-inspired techniques introduced by prior works. Moreover, our SInusoidal Gegenbauer NETwork, or SIGNET, can represent light field scenes more compactly than the state-of-the-art compression methods while maintaining a comparable reconstruction quality. SIGNET also innately allows random access to encoded light field pixels due to its functional design. We further demonstrate that SIGNET's super-resolution capability without any additional training.</p>

<h2 id="downloads">Downloads</h2>

<div style="display: flex; text-align:center; flex-direction: row; flex-wrap: wrap;">
<div style="margin:1rem; flex-grow: 1;"><a href="https://brandonyfeng.github.io/papers/SIGNET.pdf"><img style="width:7rem;height:7rem;" src="resources/doc.png" /><br /><label>Paper</label></a><br /></div>
<div style="margin:1rem; flex-grow: 1;"><a href="https://brandonyfeng.github.io/papers/ICCV_supp.pdf"><img style="width:7rem; height:7rem;" src="resources/doc.png" /><br />Supplementary</a></div>
<div style="margin:1rem; flex-grow: 1;"><a href="https://github.com/AugmentariumLab/SIGNET"><img style="width:7rem;height:7rem;" src="resources/github_logo.png" /><br />Code</a></div>
</div>

<h2 id="citation">Citation</h2>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Feng2021SIGNET</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Feng, Brandon Y. and Varshney, Amitabh}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{International Conference on Computer Vision (ICCV 2021)}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{SIGNET: Efficient Neural Representations for Light Fields}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Brandon Y. Feng and Amitabh Varshney. SIGNET: Efficient Neural Representations for Light Fields. International Conference on Computer Vision (ICCV 2021). 2021.</p>

      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
